{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:34:32.477590Z",
     "start_time": "2024-11-02T11:34:29.433094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from frames import Frames\n",
    "from audio import Audio\n",
    "from label_map import label_map\n",
    "\n",
    "import torch\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class OriginalMultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    加载原始的多模态数据。\n",
    "    \"\"\"\n",
    "    def __init__(self, is_need_audio=True, path_config_path_str='../configs/path.yaml'):\n",
    "        # 导入配置。\n",
    "        self.path_config = OmegaConf.load(path_config_path_str)\n",
    "\n",
    "        # 加载视频、字幕、音频的路径。\n",
    "        self.base_dir = Path(self.path_config['datasets']['base_dir'])\n",
    "        self.base_video_dir = Path(self.path_config['datasets']['base_video_dir'])\n",
    "        self.base_subtitle_dir = Path(self.path_config['datasets']['base_subtitle_dir'])\n",
    "        self.base_audio_dir = Path(self.path_config['datasets']['base_audio_dir'])\n",
    "\n",
    "        # 导入主控制文件。\n",
    "        self.is_need_audio = is_need_audio\n",
    "        self.all_data = pd.read_json(self.path_config['datasets']['base_all_data'], dtype={'video_id': str})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames_data = self.get_frames_data(self.all_data.loc[idx, 'video_id'])\n",
    "        result = {\n",
    "            'title': self.all_data.loc[idx, 'title'],\n",
    "            'emotion_name': self.all_data.loc[idx, 'emotion'],\n",
    "            'emotion': label_map[self.all_data.loc[idx, 'emotion']],\n",
    "            'scenes': frames_data['images'],  # 这是一个list。\n",
    "            'subtitles': frames_data['subtitles'],  # 这是一个list。\n",
    "        }\n",
    "        if self.is_need_audio:\n",
    "            audio_data = self.get_audio_data_dict(self.all_data.loc[idx, 'video_id'])\n",
    "            result = result | audio_data\n",
    "        return result\n",
    "\n",
    "    def get_frames_data(self, video_id):\n",
    "        frames = Frames(video_id)\n",
    "        video_info = frames.get_video_info()\n",
    "        frames_image = frames.get_frame_image_by_time()\n",
    "        frames_subtitle = frames.get_frame_subtitle_by_time()\n",
    "        return {\n",
    "            'images': frames_image,\n",
    "            'subtitles': frames_subtitle,\n",
    "        }\n",
    "\n",
    "    def get_audio_data_dict(self, video_id):\n",
    "        audio = Audio(video_id)\n",
    "        waveform, sample_rate = audio.load_audio()\n",
    "        return {\n",
    "            'audio_waveform': waveform,\n",
    "            'audio_sample_rate': sample_rate\n",
    "        }\n",
    "\n"
   ],
   "id": "3d3ac5bdef293792",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-02T11:38:33.856213Z",
     "start_time": "2024-11-02T11:38:33.839869Z"
    }
   },
   "source": [
    "# from .original_dataset import OriginalMultimodalDataset\n",
    "\n",
    "from embedding import TextEncoder, Captioner, ImageEncoder, FaceExtractor, AudioEncoder\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "\n",
    "class ProcessedMultimodalDataset(OriginalMultimodalDataset):\n",
    "    def __init__(self, is_need_caption=True, is_need_audio=True, path_config_path_str='../configs/path.yaml'):\n",
    "        super().__init__(is_need_audio, path_config_path_str)\n",
    "        self.is_need_caption = is_need_caption\n",
    "        # self.image_transform = image_transform\n",
    "\n",
    "        # 定义好的一系列的处理和编码器。\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.captioner = Captioner()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.face_extractor = FaceExtractor()\n",
    "        self.audio_encoder = AudioEncoder()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = super().__getitem__(idx)\n",
    "        result = {\n",
    "            'emotion': torch.tensor(data['emotion'], dtype=torch.long),\n",
    "            # 'title': data['title'],\n",
    "            'title_embedding': self.text_encoder.encode(data['title']),\n",
    "            'scene_embedding_list': self.get_scene_embedding_list(data['scenes']),\n",
    "            'face_embedding_list': self.get_face_embedding_list(data['scenes']),\n",
    "            'text_embedding_list': self.get_text_embedding_list(data['scenes'], data['subtitles']),\n",
    "            # 'audio': data['audio'],\n",
    "        }\n",
    "        if self.is_need_audio:\n",
    "            audio_embedding = self.get_audio_embedding((data['audio_waveform'], data['audio_sample_rate']))\n",
    "            result = result | audio_embedding\n",
    "        return result\n",
    "\n",
    "    def get_scene_embedding_list(self, scenes):\n",
    "        return [self.get_scene_embedding(scene) for scene in scenes]\n",
    "\n",
    "    def get_face_embedding_list(self, scenes):\n",
    "        return [self.get_face_embedding(scene) for scene in scenes]\n",
    "\n",
    "    def get_text_embedding_list(self, scenes, subtitles):\n",
    "        text_embeddings_list = []\n",
    "        for i in range(len(subtitles)):\n",
    "            scene = scenes[i]\n",
    "            subtitle = subtitles[i]\n",
    "            text_embeddings_list.append(self.get_text_embedding(scene, subtitle))\n",
    "\n",
    "        return text_embeddings_list\n",
    "\n",
    "    def transform_image(self):\n",
    "        \"\"\"将图片进行处理转换。主要是resize。\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_title_embedding(self, title):\n",
    "        \"\"\"获取标题的embedding。\"\"\"\n",
    "        return self.text_encoder.encode(title)\n",
    "\n",
    "    def get_text_embedding(self, scene, subtitle, is_need_caption=True):\n",
    "        \"\"\"获得text部分的embedding。会输入conditioned_text_encoder。\"\"\"\n",
    "        caption = self.captioner.generate(scene)\n",
    "        result = ''\n",
    "        if self.is_need_caption:\n",
    "            # 这里对于text的部分选择的方法是拼接。\n",
    "            result = subtitle + '\\n' + caption\n",
    "        else:\n",
    "            result = subtitle\n",
    "        text_embedding = self.text_encoder.encode(result)\n",
    "        return text_embedding\n",
    "\n",
    "    def generate_caption(self, np_array_image):\n",
    "        \"\"\"输入ndarray图片，输出text的caption\"\"\"\n",
    "        return self.captioner.generate(np_array_image)\n",
    "\n",
    "    # def from_image_get_caption_list(self, np_array_image_list):\n",
    "    #     \"\"\"根据本dataset的设计，输入scene list，得到对应的caption list。\"\"\"\n",
    "    #     return [self.generate_caption(np_array_image) for np_array_image in np_array_image_list]\n",
    "\n",
    "    # def get_image_embedding(self):\n",
    "    #     \"\"\"获取image部分的embedding。会输入conditioned_image_encoder。\"\"\"\n",
    "\n",
    "    def get_faces_and_ratios_list(self, np_array_image):\n",
    "        \"\"\"\n",
    "        输入np_array的图片，返回一个元组的list，分别是(face_pil_image,face_area_ration)。\n",
    "        这里默认输入的是scene。\n",
    "        \"\"\"\n",
    "        faces_with_ratios_list = self.face_extractor.extract_face(np_array_image)\n",
    "        return faces_with_ratios_list\n",
    "\n",
    "    def get_face_embedding(self, scene, is_need_norm=False):\n",
    "        \"\"\"聚合多张脸的语义信息。返回结果是(num_faces,face_embedding)\"\"\"\n",
    "        faces_with_ratios_list = self.get_faces_and_ratios_list(scene)\n",
    "        num_faces = len(faces_with_ratios_list)\n",
    "\n",
    "        total_ratio = sum(face_area_ratio for _, face_area_ratio in faces_with_ratios_list)\n",
    "        weighted_embeddings = []\n",
    "        for face_image, face_area_ratio in faces_with_ratios_list:\n",
    "            # 先将脸部的图片进行编码。\n",
    "            face_embedding = self.image_encoder.encode(face_image)\n",
    "            if is_need_norm:\n",
    "                # 这里如果需要进行归一化，就调整原本脸的占比的数值。\n",
    "                face_area_ratio = face_area_ratio / total_ratio\n",
    "            weighted_embedding = face_embedding * face_area_ratio\n",
    "            weighted_embeddings.append(weighted_embedding)\n",
    "\n",
    "        face_embedding = torch.sum(torch.stack(weighted_embeddings), dim=0)\n",
    "\n",
    "        return num_faces, face_embedding\n",
    "\n",
    "    def get_scene_embedding(self, scene):\n",
    "        return self.image_encoder.encode(scene)\n",
    "\n",
    "    def get_audio_embedding(self, audio):\n",
    "        \"\"\"获取audio部分的embedding。直接输入最终的decision模块。需要判断是否\"\"\"\n",
    "        return {'audio_embedding': self.audio_encoder.encode(audio)}\n",
    "\n",
    "    def build_image_transform(self):\n",
    "        \"\"\"默认的自建图片transform pipeline。\"\"\"\n",
    "\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:38:54.874481Z",
     "start_time": "2024-11-02T11:38:37.577068Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = ProcessedMultimodalDataset()",
   "id": "1baa5cabae764eaf",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:40:48.969107Z",
     "start_time": "2024-11-02T11:40:02.496629Z"
    }
   },
   "cell_type": "code",
   "source": "data1 = dataset[0]",
   "id": "aa592edf254ed08f",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:40:59.264251Z",
     "start_time": "2024-11-02T11:40:59.194131Z"
    }
   },
   "cell_type": "code",
   "source": "data1",
   "id": "22a735182310b968",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:47:29.223766Z",
     "start_time": "2024-11-02T11:47:29.216691Z"
    }
   },
   "cell_type": "code",
   "source": "data1['title_embedding'].shape",
   "id": "c8a91beefd0e47fb",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:49:29.320547Z",
     "start_time": "2024-11-02T11:49:29.312623Z"
    }
   },
   "cell_type": "code",
   "source": "len(data1['scene_embedding_list']), len(data1['face_embedding_list']), len(data1['text_embedding_list'])",
   "id": "f4a84bd5d5452245",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:48:59.837095Z",
     "start_time": "2024-11-02T11:48:59.830840Z"
    }
   },
   "cell_type": "code",
   "source": "data1['scene_embedding_list'][0].shape",
   "id": "e10a25145628691c",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:49:54.448057Z",
     "start_time": "2024-11-02T11:49:54.443998Z"
    }
   },
   "cell_type": "code",
   "source": "data1['face_embedding_list'][0][1].shape",
   "id": "ef25e92a1594ea1e",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:50:27.448270Z",
     "start_time": "2024-11-02T11:50:27.441085Z"
    }
   },
   "cell_type": "code",
   "source": "data1['text_embedding_list'][0].shape",
   "id": "e013070908ede69",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T11:51:55.363120Z",
     "start_time": "2024-11-02T11:51:55.358826Z"
    }
   },
   "cell_type": "code",
   "source": "data1['audio_embedding'].shape",
   "id": "d8cfd2ae77603645",
   "execution_count": 23,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
